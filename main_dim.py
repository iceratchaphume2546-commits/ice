import os
import pandas as pd
import requests
from datetime import datetime
import pytz
from google.cloud import storage
import re

# ‚ùå ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á load_dotenv ‡πÉ‡∏ô Cloud Run
# from dotenv import load_dotenv
# load_dotenv()

GCS_BUCKET_NAME = os.environ["GCS_BUCKET_NAME"]
TENANT_ID = os.environ["TENANT_ID"]
CLIENT_ID = os.environ["CLIENT_ID"]
CLIENT_SECRET = os.environ["CLIENT_SECRET"]
DATAVERSE_URL = os.environ["DATAVERSE_URL"]
SCOPE = os.environ["SCOPE"]


# -----------------------------
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ path ‡∏Ç‡∏≠‡∏á JSON key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GCS
# -----------------------------
# ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå gcp-key.json ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô container /app
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/app/gcp-key.json"

# -----------------------------
# Environment variables
# -----------------------------
GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "hongthai")
TENANT_ID = os.getenv("TENANT_ID")
CLIENT_ID = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
DATAVERSE_URL = os.getenv("DATAVERSE_URL")
SCOPE = os.getenv("SCOPE")  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô https://yourorg.crm.dynamics.com/.default

# -----------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏ö‡∏ö Bangkokk
# -----------------------------
def now_th(fmt=None):
    tz = pytz.timezone("Asia/Bangkok")
    now = datetime.utcnow().replace(tzinfo=pytz.utc).astimezone(tz)
    return now.strftime(fmt) if fmt else now

def now_th_iso():
    tz = pytz.timezone("Asia/Bangkok")
    now = datetime.now(tz)
    return now.strftime("%Y-%m-%dT%H:%M:%SZ")

# -----------------------------
# ‡∏Ç‡∏≠ access token ‡∏à‡∏≤‡∏Å Azure ADd
# -----------------------------
def get_access_token():
    url = f"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token"
    data = {
        "grant_type": "client_credentials",
        "client_id": CLIENT_ID,
        "client_secret": CLIENT_SECRET,
        "scope": SCOPE
    }
    headers = {"Content-Type": "application/x-www-form-urlencoded"}
    r = requests.post(url, data=data, headers=headers)
    r.raise_for_status()
    return r.json()["access_token"]

# -----------------------------
# ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Dataverse (FULL LOAD)
# -----------------------------
def fetch_dataverse_data(token, api_name):
    url = f"{DATAVERSE_URL}/api/data/v9.2/{api_name}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }
    data = []
    while url:
        r = requests.get(url, headers=headers)
        r.raise_for_status()
        js = r.json()
        data.extend(js.get("value", []))
        url = js.get("@odata.nextLink")
    return data

# -----------------------------
# ‡∏ó‡∏≥ column ‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BigQuery
# -----------------------------
def clean_columns_for_bq(df):
    df.columns = [re.sub(r"[^\w]", "_", c).lower() for c in df.columns]
    df.columns = [
        c if c[0].isalpha() or c[0] == "_" else f"col_{i}"
        for i, c in enumerate(df.columns)
    ]
    return df

# -----------------------------
# Upload GCS
# -----------------------------
def upload_to_gcs(df, folder, filename):
    client = storage.Client()  # ‡∏à‡∏∞‡πÉ‡∏ä‡πâ key ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
    bucket = client.bucket(GCS_BUCKET_NAME)
    path = f"{folder}/{filename}"
    blob = bucket.blob(path)

    # Save temp file ‡πÅ‡∏•‡πâ‡∏ß upload
    temp_file = "temp_dim.ndjson"
    df.to_json(temp_file, orient="records", lines=True, force_ascii=False)
    blob.upload_from_filename(temp_file)

    print(f" ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‚Üí gs://{GCS_BUCKET_NAME}/{path}")

# -----------------------------
# MAIN (FULL LOAD DIM)
# -----------------------------
if __name__ == "__main__":
    print(" ‡πÄ‡∏£‡∏¥‡πà‡∏° FULL LOAD DIM")

    # ‡∏Ç‡∏≠ access token
    token = get_access_token()

    # List ‡∏Ç‡∏≠‡∏á DIM entities
    dim_entities = {
        "dimension/channels": "itsm_channels",
        "dimension/kols": "itsm_kols",
        "dimension/pages": "itsm_pages",
        "dimension/products": "itsm_products"
    }

    # Loop ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ entityy
    for folder, api_name in dim_entities.items():
        print(f"\nüì• ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {api_name}")
        data = fetch_dataverse_data(token, api_name)

        df = pd.DataFrame(data)
        if df.empty:
            print(" ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
            continue

        df = clean_columns_for_bq(df)
        upload_to_gcs(df, folder, f"{folder.split('/')[-1]}.ndjson")

    print("üéâ FULL LOAD DIM ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
